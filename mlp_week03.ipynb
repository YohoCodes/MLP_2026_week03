{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNnuZONqYsb4"
   },
   "source": [
    "# Week 3: Clustering\n",
    "\n",
    "**Student Name 1, Student Name 2**\n",
    "\n",
    "In this workshop, we will work through a set of problems on clustering, another cannonical form of unsupervised learning. Clustering is an important tool that is used to discover homogeneous groups of data points within a heterogeneous population. It can be the main goal in some problems, while in others it may be used in EDA to understand the main types of behavior in the data or in feature engineering.   \n",
    "\n",
    "We will start by generating some artificial data, and then we will utilize clustering algorithms described in lectures and explore the impact of feature engineering on the solution. We will then attempt to find clusters in a gene expression dataset. \n",
    "\n",
    "As usual, the worksheets will be completed in teams of 2-3, using **pair programming**, and we have provided cues to switch roles between driver and navigator. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- Look for the üèÅ as cue to switch roles between driver and navigator.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, **you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday** of the week the workshop was given. \n",
    "\n",
    "As you work through the problems it will help to refer to your lecture notes (navigator). The exercises here are designed to reinforce the topics covered this week. Please discuss with the tutors if you get stuck, even early on! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Problem Definition and Setup: Simulated Example](#setup1)\n",
    "2. [K-means: Simulated Example](#kmeans1)\n",
    "3. [Hierarchical Clustering: Simulated Example](#hc)\n",
    "4. [Gene Expression Data](#genedata)\n",
    "5. [Hierarchical Clustering: Gene Expression Data](#hc_genedata)\n",
    "6. [K-means Clustering: Gene Expression Data](#kmeans_genedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAdywZYoZa5T"
   },
   "source": [
    "# Problem Definition and Setup: Simulated Example <a id='setup1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "First, lets load in some packages to get us started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "am3xGa8BYohT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWAWprSdZ11V"
   },
   "source": [
    "## Data: Simulated Example\n",
    "\n",
    "We will begin with a simple simulated example in which there are truly three clusters. We assume that there are $D=2$ features and within each cluster, the data points are generated from a spherical normal distribution $N(\\mathbf{m}_k, \\sigma^2_k \\mathbf{I})$ for clusters $k=1,2,3$, where both the mean $\\mathbf{m}_k$ and variance $\\sigma^2_k$ are different across clusters. Specifically, we assume that: \n",
    "\n",
    "* Cluster 1: contains $|C_1|=500$ points with mean vector $\\mathbf{m}_1 = \\begin{pmatrix} 0 \\\\ 4 \\end{pmatrix}$ with standard deviation $\\sigma_1 = 2$.\n",
    "* Cluster 2: contains $|C_2|=250$ points with mean vector $\\mathbf{m}_2 = \\begin{pmatrix} 0 \\\\ -4 \\end{pmatrix}$ with standard deviation $\\sigma_2 = 1$.\n",
    "* Cluster 3: contains $|C_3|=100$ points with mean vector $\\mathbf{m}_3 = \\begin{pmatrix} -4 \\\\ 0 \\end{pmatrix}$ with standard deviation $\\sigma_3 = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to generate the dataset described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "BzWG5w4SZv8q"
   },
   "outputs": [],
   "source": [
    "# Number of features\n",
    "D = 2\n",
    "\n",
    "# Cluster sizes\n",
    "N_1 = 500\n",
    "N_2 = 250\n",
    "N_3 = 100\n",
    "\n",
    "# Cluster means\n",
    "m_1 = np.array([0., 4.])\n",
    "m_2 = np.array([0., -4.])\n",
    "m_3 = np.array([-4., 0.])\n",
    "\n",
    "# Cluster standard deviations\n",
    "sd_1 = 2.\n",
    "sd_2 = 1.\n",
    "sd_3 = 0.5\n",
    "\n",
    "# Generate the data\n",
    "rnd = np.random.RandomState(5)\n",
    "X_1 = rnd.normal(loc = m_1, scale = sd_1, size = (N_1,D))\n",
    "X_2 = rnd.normal(loc = m_2, scale = sd_2, size = (N_2,D))\n",
    "X_3 = rnd.normal(loc = m_3, scale = sd_3, size = (N_3,D))\n",
    "X = np.vstack((X_1, X_2, X_3))\n",
    "\n",
    "# Save true cluster labels\n",
    "cl = np.hstack((np.repeat(1,N_1),np.repeat(2,N_2),np.repeat(3,N_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the size is correct\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 1 (CORE)\n",
    "\n",
    "Visualise the data and color by the true cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "mKM3uECSblEP",
    "outputId": "5327b071-bad4-4362-e404-17e67da71d98"
   },
   "outputs": [],
   "source": [
    "# Code for answer here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering: Simulated Example <a id='kmeans1'></a>\n",
    "\n",
    "To perform K-means clustering, we will use `KMeans()` in `sklearn.cluster`. Documentation is available [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and for an overview of clustering methods available in `sklearn`, see [link](https://scikit-learn.org/stable/modules/clustering.html). There are different inputs we can specify when calling `KMeans()` such as:\n",
    "\n",
    "- `n_clusters`: the number of clusters. \n",
    "- `init`: which specifies the intialization of the centroids, e.g. can be set to `k-means++` for K-means++ initialization or `random` for random initialization.\n",
    "- `n_init`: which specifies the number of times the algorithm is run with different random initializations\n",
    "- `random_state`: this can bet set to a fixed number to make results reproducible.\n",
    "\n",
    "We can then use the `.fit()` method of `KMeans` to run the K-means algorithm on our data.\n",
    "\n",
    "After fitting, some of the relevant attributes of interest include:\n",
    "\n",
    "- `labels_`: cluster assignments of the data points.\n",
    "- `cluster_centers_`: mean corresponding to each cluster, stored in a matrix of size: number of clusters $K$ times number features $D$.\n",
    "- `inertia_`: the total within-cluster variation.\n",
    "\n",
    "We can also call the methods `.transform()` and `.predict()` on our fitted `KMeans()` objects. The former transforms/encodes an $N \\times D$ feature matrix into an $N \\times C$ matrix, where the new features reprent the distance to cluster $c$, for $c=1,\\ldots, C$. The later predicts the cluster labels of each sample in  an $N \\times D$ feature matrix (i.e. returns the index of the closest cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 2 (CORE)\n",
    "\n",
    "Let's start by exploring how the clustering changes across the K-means iterations. To do, set:\n",
    "\n",
    "- number of clusters to 3\n",
    "- initialization to random\n",
    "- number of times the algorithm is run to 1\n",
    "- fix the random seed to a number of your choice (e.g. 0)\n",
    "\n",
    "\n",
    "a) Now, fit the K-means algorithms with different values of the maximum number of iterations fixed to 1,2,3, and the default value of 300. \n",
    "\n",
    "b) Plot the data points colored by cluster for the four different cases and mark the cluster centers to observe how the clustering solution changes across iterations. \n",
    "\n",
    "c) How many iterations are needed for the convergence?\n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- To find the number of iterations, check the attributes of [`KMeans`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html)\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a: Code for answer here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b: Code for answer here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c: Code for answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 3 (CORE)\n",
    "\n",
    "Next, compare the random intialization with K-means++ (in this case fix the number of different initializations to 10). Plot both clustering solutions. Which requires fewer iterations? and which provides a lower within-cluster variation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 4 (CORE)\n",
    "\n",
    "In the following two code cells, we compare the clustering solution using a different number of initializations equal to 1, 2, 5, 10, and 20 for kmeans++ initialization (first cell) and random initialization (second cell). \n",
    "\n",
    "Based on the results, comment on the preferred initialization strategy and how many intializations are needed? Try changing the random state; how does that change your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans++ initialization\n",
    "# Plotting the clustering solution with different number of initializations \n",
    "n_init = np.array([1,2,5,10,20])\n",
    "\n",
    "rs = 0\n",
    "\n",
    "fig, ax = plt.subplots(1,n_init.shape[0],figsize=(20,4))\n",
    "for n in range(n_init.shape[0]):\n",
    "    kmeans_n = KMeans(n_clusters = 3, n_init = n_init[n], random_state=rs).fit(X)\n",
    "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=kmeans_n.labels_, palette='Set1', s=10, ax=ax[n])\n",
    "    sns.scatterplot(x=kmeans_n.cluster_centers_[:,0], y=kmeans_n.cluster_centers_[:,1], \n",
    "                    c='black', s=50, marker='X', ax=ax[n])\n",
    "    ax[n].set_title(\"No. of Init=\"+str(n_init[n]))\n",
    "plt.show()\n",
    "\n",
    "# Print the within cluster variation\n",
    "for n in range(n_init.shape[0]):\n",
    "    kmeans_n = KMeans(n_clusters = 3, n_init = n_init[n], random_state=rs).fit(X)\n",
    "    print(\"WCV=\"+str(round(kmeans_n.inertia_,4))+' for no. of init ='+ str(n_init[n]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialization\n",
    "# Plotting the clustering solution with different number of initializations \n",
    "n_init = np.array([1,2,5,10,20])\n",
    "\n",
    "rs = 0\n",
    "\n",
    "fig, ax = plt.subplots(1,n_init.shape[0],figsize=(20,4))\n",
    "for n in range(n_init.shape[0]):\n",
    "    kmeans_n = KMeans(n_clusters = 3, init = 'random', n_init = n_init[n], random_state=rs).fit(X)\n",
    "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=kmeans_n.labels_, palette='Set1', s=10, ax=ax[n])\n",
    "    sns.scatterplot(x=kmeans_n.cluster_centers_[:,0], y=kmeans_n.cluster_centers_[:,1], \n",
    "                    c='black', s=50, marker='X', ax=ax[n])\n",
    "    ax[n].set_title(\"No. of Init=\"+str(n_init[n]))\n",
    "plt.show()\n",
    "\n",
    "# Print the within cluster variation\n",
    "for n in range(n_init.shape[0]):\n",
    "    kmeans_n = KMeans(n_clusters = 3, init = 'random',n_init = n_init[n], random_state=rs).fit(X)\n",
    "    print(\"WCV=\"+str(round(kmeans_n.inertia_,4))+' for no. of init ='+ str(n_init[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 5 (CORE)\n",
    "\n",
    "Since we simulated the data, we know the true number of clusters. However, in practice this number is rarely known. Find the K-means solution with different choices of $K$ and plot the within-cluster variation as a function of $K$. What value(s) of $K$ seem appropriate based on this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code for answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 6 (CORE)\n",
    "\n",
    "Now let's use the silhouette analysis to choose the number of clusters. In the follwing code cells, we use [`silhouette_samples`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html) to compute the silhouette coefficient for each data point, and [`silhouette_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) which computes the mean of the silhouette coefficients across all data points.\n",
    "\n",
    "a) How many clusters would you choose based only on maximizing the silhouette score?\n",
    "\n",
    "b) Considering also the violin plots, which visualize the distribution of the silhouette coefficient across data points within each cluster, would you still choose the same number of clusters or pick a different number? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Define the range of possible number ofclusters\n",
    "K = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# First plot the silhouette coeffcient for different choices of K\n",
    "silhouette_coeffs = np.zeros(K.shape)\n",
    "for i in range(K.shape[0]):\n",
    "    #Define kmeans object, fit, and predict labels\n",
    "    kmeans_K = KMeans(n_clusters = K[i], n_init = 20, random_state=0).fit(X)\n",
    "    labs = kmeans_K.predict(X)\n",
    "    # Compute silhouette coefficient\n",
    "    silhouette_coeffs[i] = silhouette_score(X, labs)\n",
    "\n",
    "# Plot the silhouette coefficients\n",
    "fig, ax = plt.subplots(1,1,figsize=(5,4))\n",
    "sns.lineplot(x=K, y=silhouette_coeffs, linestyle='dashed', ax=ax)\n",
    "sns.scatterplot(x=K, y=silhouette_coeffs, ax=ax)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the silhouette samples for different choices of K\n",
    "\n",
    "max_i = 4\n",
    "fig, ax = plt.subplots(2, 5, figsize=(25, 10))  \n",
    "for i in range(max_i+1):\n",
    "    kmeans_K = KMeans(n_clusters = K[i], n_init = 20, random_state=0).fit(X)\n",
    "    labs = kmeans_K.predict(X)\n",
    "    silhouette_samps = silhouette_samples(X, labs)\n",
    "\n",
    "    # Create a violin plot of the silhouette samples\n",
    "    sns.violinplot(y=silhouette_samps, hue=labs, ax=ax[0,i], palette='Set1')\n",
    "    ax[0,i].axhline(silhouette_samps.mean(), ls='--',c=\"gray\")\n",
    "    ax[0,i].set_title(f'Silhouette Samples for K={K[i]}')\n",
    "    ax[0,i].set_ylabel('Silhouette Coefficient')\n",
    "    ax[0,i].set_xlabel('')\n",
    "    ax[0,i].set_ylim([-0.2, 1])\n",
    "\n",
    "    # Plot the clustering solution\n",
    "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=labs, ax=ax[1,i], palette='Set1')\n",
    "    sns.scatterplot(x=kmeans_K.cluster_centers_[:,0], y=kmeans_K.cluster_centers_[:,1], \n",
    "                    c='black', s=50, marker='X', ax=ax[1,i])\n",
    "    ax[1,i].set_title(f'Clustering Solution for K={K[i]}')\n",
    "    ax[1,i].set_ylabel('X2')\n",
    "    ax[1,i].set_xlabel('X1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 7 (CORE)\n",
    "\n",
    "Now standardize the data and re-run the K-means algorithm. Qualitatively, how has standardising the data impacted performance? Can you argue why you observe what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering: Simulated Example <a id='hc'></a>\n",
    "\n",
    "To perform hierarchical clustering, we will use the [`linkage()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) function from `scipy.cluster.hierarchy`. The inputs to specify include\n",
    "\n",
    "-  the data. \n",
    "- `metric`: specifies the dissimarlity between data points. Defaults to the Euclidean distance.\n",
    "- `method`: specifies the type of linkage, e.g. complete, single, or average.\n",
    "\n",
    "Then, we can use [`dendrogram()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html) from `scipy.cluster.hierarchy` to plot the dendrogram.\n",
    "\n",
    "Note that you can also use [`AgglomerativeClustering`](https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.AgglomerativeClustering.html) from `sklearn.cluster`, which similarly has options for `metric` to specify the distance and `linkage` to specify the type of linkage. However, `sklearn` does not have its own functions for plotting the dendogram and use must use the tools from `scipy.cluster.hierarchy`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 8 (CORE)\n",
    "\n",
    "a) For the following code cell, what is dissimarlity and linkage is used in hierarchical clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_comp = hierarchy.linkage(X, method='complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)  Plot the dendogram by running the code below. Try changing the 'color_threshold' to a number (e.g. 11) to color the branches of the tree below the threshold with different colors. How many clusters are identified if the tree is cut at that threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dendrogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "hierarchy.dendrogram(hc_comp, ax=ax, no_labels=True,\n",
    "                     color_threshold=-np.inf, above_threshold_color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Now, use the function `cut_tree()` from `scipy.cluster.hierarchy` to determine the cluster labels associated with a given cut of the dendrogram. You can either specify the number of clusters via `n_clusters` or the height/threshold at which to cut via `height`. Plot the data colored by the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your aswer here: Cut the tree at a specified number of clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 9 (CORE)\n",
    "\n",
    "Now try changing the linkage to single and average. Does this affect on the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for answer here\n",
    "# First: change to single linkage, plot the dendrogram, and visualize the clustering solution by cutting the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for answer here\n",
    "# Next: change to average linkage, plot the dendrogram, and visualize the clustering solution by cutting the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG5WyUN2GbiB"
   },
   "source": [
    "# Gene Expression Data <a id='genedata'></a>\n",
    "\n",
    "Now, we will consider a more complex real dataset with a larger feature space. \n",
    "\n",
    "The dataset is the **NCI cancer microarray dataset** discussed in both *Introduction to Statistical Learning* and  *Elements of Statistical Learning*. The dataset consists of $D=6830$ gene expression measurements for each of $N=64$ cancer cell lines. The aim is to determine whether there are groups among the cell lines with similar gene expression patterns. This is an example of a high-dimensional dataset with $D$ much larger than $N$, which makes visualization difficult. The $N=64$ cancer cell lines have been obtained from samples of cancerous tisses, corresponding to 14 different types of cancer. However, our focus remains unsupervised learning and we will use the cancer labels only to plot. \n",
    "\n",
    "We first need to read in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leJH2StnFPaI"
   },
   "outputs": [],
   "source": [
    "#Fetch the data and cancer labels\n",
    "url_data = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.data.csv'\n",
    "url_labels = 'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/nci.label.txt'\n",
    "\n",
    "X = pd.read_csv(url_data)\n",
    "y = pd.read_csv(url_labels, header=None)\n",
    "\n",
    "# clean data by dropping identifier column and transpose so that features are columns\n",
    "X = X.drop(labels='Unnamed: 0', axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H97lGxLsOa9v"
   },
   "source": [
    "Let's visualise the data with a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Tb1J3rLMNula",
    "outputId": "0eb88c8e-0415-4be5-f646-1b677105071c"
   },
   "outputs": [],
   "source": [
    "# Heatmap of the gene expression data\n",
    "fig, ax = plt.subplots(1,1,figsize=(30,5))\n",
    "sns.heatmap(X, cmap='inferno', ax=ax)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"Gene expression\", fontsize=22)\n",
    "ax.set_ylabel(\"Tissue sample\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mozq7WKja2gW"
   },
   "source": [
    "We now convert our pandas dataframe into a numpy array and create integer labels for cancer type (for plotting purposes)\n",
    "\n",
    "If you print the unique labels, you will notice there are lots of inconsistencies with white space etc. Run the following code to clean the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique labels and counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the labels by stripping the white space\n",
    "y_clean = np.asarray(y).flatten()\n",
    "for j in range(y_clean.size):\n",
    "    y_clean[j] = y_clean[j].strip()\n",
    "\n",
    "cancer_types = list(np.unique(y_clean))\n",
    "cancer_groups = np.array([cancer_types.index(lab) for lab in y_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_clean).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13qbb2jqbvhT"
   },
   "source": [
    "### üö© Exercise 10 (EXTRA)\n",
    "\n",
    "Perform a PCA of $\\mathbf X$ to visualize the data. Plot the first few principal component scores and color by cancer type. Do cell lines within the same cancer types seems to have similar scores? Make a scree plot of the proportion of variance explained. How many components does this suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEdWUy88bYMi"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Code for answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering: Gene Expression Data <a id='hc_genedata'></a>\n",
    "\n",
    "Now, let's perform hierarchical clustering on the gene expression data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 11 (CORE)\n",
    "\n",
    "a) Plot the dendrogram with complete, single, and average linkage. Does the choice of linkage affect the results? Which linkage would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!\n",
    "# Fit hierarchical clustering with different types of linkage\n",
    "\n",
    "\n",
    "# Plot the dendogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Select a linkage and a number of clusters (by examining the dendrogram and jumps in the heights of the clusters merged). Plot the dendogram and color the branches to identify the clusters. Use the option `labels = np.asarray(y_clean), leaf_font_size=10` in `hierarchy.dendrogram` to add the cancer types as labels for each data point. Do you observe any patterns between the clusters and cancer types? You may also want to use `pd.crosstab` to compute a cross-tabulation to compare the clusters and cancer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means Clustering: Gene Expression Data <a id='kmeans_genedata'></a>\n",
    "\n",
    "Now, let's perform k-means clustering on the gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 12 (CORE)\n",
    "\n",
    "Perform K-means clustering with the same number of clusters that you selected for hierarchical clustering. Are the results similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Exercise 13 (EXTRA)\n",
    "\n",
    "Plot the two clustering solutions along with a plot of the data colored by the cancer types in the space spanned by the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So3kpRjk2-yx"
   },
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please **change 'Student 1' and 'Student 2' at the top of the notebook to include your name(s)**.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week03.ipynb "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "title": "MLPy Workshop 3"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
